{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义数据集\n",
    "\n",
    "自定义输入X为7张高和宽均为2像素的灰度图片  \n",
    "自定义输出target为 $y_1$=0, $y_2$=1, $y_3$=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练数据和标签\n",
    "X = torch.tensor([[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [5.8, 2.6, 4.0, 1.2], [6.7, 3.0, 5.2, 2.3]], dtype=torch.float32).to(device)\n",
    "target = torch.tensor([0, 0, 1, 2], dtype=torch.long).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络结构\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc = nn.Linear(4, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = LinearNet().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "` torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') `\n",
    "衡量模型输出与真实标签的差异，在分类时相当有用。  \n",
    "结合了` nn.LogSoftmax() `和` nn.NLLLoss() `两个函数，进行交叉熵计算。  \n",
    "主要参数：\n",
    "weight: 各类别的loss设置权值  \n",
    "ignore_index: 忽略某个类别  \n",
    "reduction: 计算模式，可为none/sum/mean  \n",
    "none: 逐个元素计算  \n",
    "sum: 所有元素求和，返回标量  \n",
    "mean: 加权平均，返回标量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义优化函数\n",
    "- torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "- 构建一个优化器对象optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数，使得模型输出更接近真实标签。\n",
    "- 学习率（learning rate）控制更新的步伐。\n",
    "- 主要参数：\n",
    "    - params: 管理的参数组\n",
    "    - lr: 初始化学习率\n",
    "    - momentum: 动量系数\n",
    "    - weight_decay: L2正则化系数\n",
    "    - nesterov: 是否采用NAG\n",
    "- zero_grad(): 清空所管理参数的梯度，因为Pytorch张量梯度不自动清零。\n",
    "- step(): 执行一步更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 随机梯度下降法\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.600651\n",
      "Epoch 100, Train loss: 0.595703\n",
      "Epoch 200, Train loss: 0.591606\n",
      "Epoch 300, Train loss: 0.588167\n",
      "Epoch 400, Train loss: 0.585245\n",
      "Epoch 500, Train loss: 0.582735\n",
      "Epoch 600, Train loss: 0.580558\n",
      "Epoch 700, Train loss: 0.578654\n",
      "Epoch 800, Train loss: 0.576976\n",
      "Epoch 900, Train loss: 0.575486\n",
      "Epoch 1000, Train loss: 0.574156\n",
      "Epoch 1100, Train loss: 0.572962\n",
      "Epoch 1200, Train loss: 0.571883\n",
      "Epoch 1300, Train loss: 0.570905\n",
      "Epoch 1400, Train loss: 0.570014\n",
      "Epoch 1500, Train loss: 0.569200\n",
      "Epoch 1600, Train loss: 0.568452\n",
      "Epoch 1700, Train loss: 0.567764\n",
      "Epoch 1800, Train loss: 0.567128\n",
      "Epoch 1900, Train loss: 0.566539\n",
      "Epoch 2000, Train loss: 0.565991\n",
      "Epoch 2100, Train loss: 0.565482\n",
      "Epoch 2200, Train loss: 0.565006\n",
      "Epoch 2300, Train loss: 0.564561\n",
      "Epoch 2400, Train loss: 0.564144\n",
      "Epoch 2500, Train loss: 0.563752\n",
      "Epoch 2600, Train loss: 0.563384\n",
      "Epoch 2700, Train loss: 0.563036\n",
      "Epoch 2800, Train loss: 0.562708\n",
      "Epoch 2900, Train loss: 0.562398\n",
      "Epoch 3000, Train loss: 0.562104\n",
      "Epoch 3100, Train loss: 0.561825\n",
      "Epoch 3200, Train loss: 0.561561\n",
      "Epoch 3300, Train loss: 0.561309\n",
      "Epoch 3400, Train loss: 0.561069\n",
      "Epoch 3500, Train loss: 0.560841\n",
      "Epoch 3600, Train loss: 0.560623\n",
      "Epoch 3700, Train loss: 0.560415\n",
      "Epoch 3800, Train loss: 0.560216\n",
      "Epoch 3900, Train loss: 0.560026\n",
      "Epoch 4000, Train loss: 0.559843\n",
      "Epoch 4100, Train loss: 0.559668\n",
      "Epoch 4200, Train loss: 0.559501\n",
      "Epoch 4300, Train loss: 0.559340\n",
      "Epoch 4400, Train loss: 0.559185\n",
      "Epoch 4500, Train loss: 0.559036\n",
      "Epoch 4600, Train loss: 0.558892\n",
      "Epoch 4700, Train loss: 0.558754\n",
      "Epoch 4800, Train loss: 0.558621\n",
      "Epoch 4900, Train loss: 0.558493\n",
      "Epoch 5000, Train loss: 0.558369\n",
      "Epoch 5100, Train loss: 0.558249\n",
      "Epoch 5200, Train loss: 0.558133\n",
      "Epoch 5300, Train loss: 0.558022\n",
      "Epoch 5400, Train loss: 0.557913\n",
      "Epoch 5500, Train loss: 0.557809\n",
      "Epoch 5600, Train loss: 0.557707\n",
      "Epoch 5700, Train loss: 0.557609\n",
      "Epoch 5800, Train loss: 0.557514\n",
      "Epoch 5900, Train loss: 0.557422\n",
      "Epoch 6000, Train loss: 0.557332\n",
      "Epoch 6100, Train loss: 0.557245\n",
      "Epoch 6200, Train loss: 0.557161\n",
      "Epoch 6300, Train loss: 0.557079\n",
      "Epoch 6400, Train loss: 0.556999\n",
      "Epoch 6500, Train loss: 0.556922\n",
      "Epoch 6600, Train loss: 0.556846\n",
      "Epoch 6700, Train loss: 0.556773\n",
      "Epoch 6800, Train loss: 0.556702\n",
      "Epoch 6900, Train loss: 0.556632\n",
      "Epoch 7000, Train loss: 0.556564\n",
      "Epoch 7100, Train loss: 0.556498\n",
      "Epoch 7200, Train loss: 0.556434\n",
      "Epoch 7300, Train loss: 0.556371\n",
      "Epoch 7400, Train loss: 0.556310\n",
      "Epoch 7500, Train loss: 0.556251\n",
      "Epoch 7600, Train loss: 0.556193\n",
      "Epoch 7700, Train loss: 0.556136\n",
      "Epoch 7800, Train loss: 0.556080\n",
      "Epoch 7900, Train loss: 0.556026\n",
      "Epoch 8000, Train loss: 0.555973\n",
      "Epoch 8100, Train loss: 0.555921\n",
      "Epoch 8200, Train loss: 0.555871\n",
      "Epoch 8300, Train loss: 0.555821\n",
      "Epoch 8400, Train loss: 0.555773\n",
      "Epoch 8500, Train loss: 0.555726\n",
      "Epoch 8600, Train loss: 0.555679\n",
      "Epoch 8700, Train loss: 0.555634\n",
      "Epoch 8800, Train loss: 0.555590\n",
      "Epoch 8900, Train loss: 0.555546\n",
      "Epoch 9000, Train loss: 0.555504\n",
      "Epoch 9100, Train loss: 0.555462\n",
      "Epoch 9200, Train loss: 0.555421\n",
      "Epoch 9300, Train loss: 0.555381\n",
      "Epoch 9400, Train loss: 0.555342\n",
      "Epoch 9500, Train loss: 0.555304\n",
      "Epoch 9600, Train loss: 0.555266\n",
      "Epoch 9700, Train loss: 0.555229\n",
      "Epoch 9800, Train loss: 0.555193\n",
      "Epoch 9900, Train loss: 0.555157\n",
      "Epoch 10000, Train loss: 0.555122\n",
      "Epoch 10100, Train loss: 0.555088\n",
      "Epoch 10200, Train loss: 0.555054\n",
      "Epoch 10300, Train loss: 0.555021\n",
      "Epoch 10400, Train loss: 0.554989\n",
      "Epoch 10500, Train loss: 0.554957\n",
      "Epoch 10600, Train loss: 0.554926\n",
      "Epoch 10700, Train loss: 0.554895\n",
      "Epoch 10800, Train loss: 0.554865\n",
      "Epoch 10900, Train loss: 0.554835\n",
      "Epoch 11000, Train loss: 0.554806\n",
      "Epoch 11100, Train loss: 0.554778\n",
      "Epoch 11200, Train loss: 0.554749\n",
      "Epoch 11300, Train loss: 0.554722\n",
      "Epoch 11400, Train loss: 0.554694\n",
      "Epoch 11500, Train loss: 0.554668\n",
      "Epoch 11600, Train loss: 0.554641\n",
      "Epoch 11700, Train loss: 0.554615\n",
      "Epoch 11800, Train loss: 0.554590\n",
      "Epoch 11900, Train loss: 0.554565\n",
      "Epoch 12000, Train loss: 0.554540\n",
      "Epoch 12100, Train loss: 0.554516\n",
      "Epoch 12200, Train loss: 0.554492\n",
      "Epoch 12300, Train loss: 0.554468\n",
      "Epoch 12400, Train loss: 0.554445\n",
      "Epoch 12500, Train loss: 0.554422\n",
      "Epoch 12600, Train loss: 0.554399\n",
      "Epoch 12700, Train loss: 0.554377\n",
      "Epoch 12800, Train loss: 0.554356\n",
      "Epoch 12900, Train loss: 0.554334\n",
      "Epoch 13000, Train loss: 0.554313\n",
      "Epoch 13100, Train loss: 0.554292\n",
      "Epoch 13200, Train loss: 0.554271\n",
      "Epoch 13300, Train loss: 0.554251\n",
      "Epoch 13400, Train loss: 0.554231\n",
      "Epoch 13500, Train loss: 0.554211\n",
      "Epoch 13600, Train loss: 0.554192\n",
      "Epoch 13700, Train loss: 0.554173\n",
      "Epoch 13800, Train loss: 0.554154\n",
      "Epoch 13900, Train loss: 0.554135\n",
      "Epoch 14000, Train loss: 0.554117\n",
      "Epoch 14100, Train loss: 0.554098\n",
      "Epoch 14200, Train loss: 0.554080\n",
      "Epoch 14300, Train loss: 0.554063\n",
      "Epoch 14400, Train loss: 0.554045\n",
      "Epoch 14500, Train loss: 0.554028\n",
      "Epoch 14600, Train loss: 0.554011\n",
      "Epoch 14700, Train loss: 0.553994\n",
      "Epoch 14800, Train loss: 0.553978\n",
      "Epoch 14900, Train loss: 0.553962\n",
      "Epoch 15000, Train loss: 0.553945\n",
      "Epoch 15100, Train loss: 0.553930\n",
      "Epoch 15200, Train loss: 0.553914\n",
      "Epoch 15300, Train loss: 0.553898\n",
      "Epoch 15400, Train loss: 0.553883\n",
      "Epoch 15500, Train loss: 0.553868\n",
      "Epoch 15600, Train loss: 0.553853\n",
      "Epoch 15700, Train loss: 0.553838\n",
      "Epoch 15800, Train loss: 0.553824\n",
      "Epoch 15900, Train loss: 0.553809\n",
      "Epoch 16000, Train loss: 0.553795\n",
      "Epoch 16100, Train loss: 0.553781\n",
      "Epoch 16200, Train loss: 0.553767\n",
      "Epoch 16300, Train loss: 0.553753\n",
      "Epoch 16400, Train loss: 0.553740\n",
      "Epoch 16500, Train loss: 0.553726\n",
      "Epoch 16600, Train loss: 0.553713\n",
      "Epoch 16700, Train loss: 0.553700\n",
      "Epoch 16800, Train loss: 0.553687\n",
      "Epoch 16900, Train loss: 0.553674\n",
      "Epoch 17000, Train loss: 0.553661\n",
      "Epoch 17100, Train loss: 0.553649\n",
      "Epoch 17200, Train loss: 0.553637\n",
      "Epoch 17300, Train loss: 0.553624\n",
      "Epoch 17400, Train loss: 0.553612\n",
      "Epoch 17500, Train loss: 0.553600\n",
      "Epoch 17600, Train loss: 0.553588\n",
      "Epoch 17700, Train loss: 0.553577\n",
      "Epoch 17800, Train loss: 0.553565\n",
      "Epoch 17900, Train loss: 0.553554\n",
      "Epoch 18000, Train loss: 0.553542\n",
      "Epoch 18100, Train loss: 0.553531\n",
      "Epoch 18200, Train loss: 0.553520\n",
      "Epoch 18300, Train loss: 0.553509\n",
      "Epoch 18400, Train loss: 0.553498\n",
      "Epoch 18500, Train loss: 0.553487\n",
      "Epoch 18600, Train loss: 0.553477\n",
      "Epoch 18700, Train loss: 0.553466\n",
      "Epoch 18800, Train loss: 0.553456\n",
      "Epoch 18900, Train loss: 0.553446\n",
      "Epoch 19000, Train loss: 0.553435\n",
      "Epoch 19100, Train loss: 0.553425\n",
      "Epoch 19200, Train loss: 0.553415\n",
      "Epoch 19300, Train loss: 0.553405\n",
      "Epoch 19400, Train loss: 0.553395\n",
      "Epoch 19500, Train loss: 0.553386\n",
      "Epoch 19600, Train loss: 0.553376\n",
      "Epoch 19700, Train loss: 0.553367\n",
      "Epoch 19800, Train loss: 0.553357\n",
      "Epoch 19900, Train loss: 0.553348\n",
      "Epoch 20000, Train loss: 0.553339\n",
      "Epoch 20100, Train loss: 0.553330\n",
      "Epoch 20200, Train loss: 0.553320\n",
      "Epoch 20300, Train loss: 0.553311\n",
      "Epoch 20400, Train loss: 0.553303\n",
      "Epoch 20500, Train loss: 0.553294\n",
      "Epoch 20600, Train loss: 0.553285\n",
      "Epoch 20700, Train loss: 0.553276\n",
      "Epoch 20800, Train loss: 0.553268\n",
      "Epoch 20900, Train loss: 0.553259\n",
      "Epoch 21000, Train loss: 0.553251\n",
      "Epoch 21100, Train loss: 0.553243\n",
      "Epoch 21200, Train loss: 0.553234\n",
      "Epoch 21300, Train loss: 0.553226\n",
      "Epoch 21400, Train loss: 0.553218\n",
      "Epoch 21500, Train loss: 0.553210\n",
      "Epoch 21600, Train loss: 0.553202\n",
      "Epoch 21700, Train loss: 0.553194\n",
      "Epoch 21800, Train loss: 0.553186\n",
      "Epoch 21900, Train loss: 0.553179\n",
      "Epoch 22000, Train loss: 0.553171\n",
      "Epoch 22100, Train loss: 0.553163\n",
      "Epoch 22200, Train loss: 0.553156\n",
      "Epoch 22300, Train loss: 0.553148\n",
      "Epoch 22400, Train loss: 0.553141\n",
      "Epoch 22500, Train loss: 0.553134\n",
      "Epoch 22600, Train loss: 0.553126\n",
      "Epoch 22700, Train loss: 0.553119\n",
      "Epoch 22800, Train loss: 0.553112\n",
      "Epoch 22900, Train loss: 0.553105\n",
      "Epoch 23000, Train loss: 0.553098\n",
      "Epoch 23100, Train loss: 0.553091\n",
      "Epoch 23200, Train loss: 0.553084\n",
      "Epoch 23300, Train loss: 0.553077\n",
      "Epoch 23400, Train loss: 0.553070\n",
      "Epoch 23500, Train loss: 0.553064\n",
      "Epoch 23600, Train loss: 0.553057\n",
      "Epoch 23700, Train loss: 0.553050\n",
      "Epoch 23800, Train loss: 0.553044\n",
      "Epoch 23900, Train loss: 0.553037\n",
      "Epoch 24000, Train loss: 0.553031\n",
      "Epoch 24100, Train loss: 0.553024\n",
      "Epoch 24200, Train loss: 0.553018\n",
      "Epoch 24300, Train loss: 0.553012\n",
      "Epoch 24400, Train loss: 0.553005\n",
      "Epoch 24500, Train loss: 0.552999\n",
      "Epoch 24600, Train loss: 0.552993\n",
      "Epoch 24700, Train loss: 0.552987\n",
      "Epoch 24800, Train loss: 0.552981\n",
      "Epoch 24900, Train loss: 0.552975\n",
      "Epoch 25000, Train loss: 0.552969\n",
      "Epoch 25100, Train loss: 0.552963\n",
      "Epoch 25200, Train loss: 0.552957\n",
      "Epoch 25300, Train loss: 0.552951\n",
      "Epoch 25400, Train loss: 0.552945\n",
      "Epoch 25500, Train loss: 0.552939\n",
      "Epoch 25600, Train loss: 0.552934\n",
      "Epoch 25700, Train loss: 0.552928\n",
      "Epoch 25800, Train loss: 0.552922\n",
      "Epoch 25900, Train loss: 0.552917\n",
      "Epoch 26000, Train loss: 0.552911\n",
      "Epoch 26100, Train loss: 0.552906\n",
      "Epoch 26200, Train loss: 0.552900\n",
      "Epoch 26300, Train loss: 0.552895\n",
      "Epoch 26400, Train loss: 0.552890\n",
      "Epoch 26500, Train loss: 0.552884\n",
      "Epoch 26600, Train loss: 0.552879\n",
      "Epoch 26700, Train loss: 0.552874\n",
      "Epoch 26800, Train loss: 0.552869\n",
      "Epoch 26900, Train loss: 0.552863\n",
      "Epoch 27000, Train loss: 0.552858\n",
      "Epoch 27100, Train loss: 0.552853\n",
      "Epoch 27200, Train loss: 0.552848\n",
      "Epoch 27300, Train loss: 0.552843\n",
      "Epoch 27400, Train loss: 0.552838\n",
      "Epoch 27500, Train loss: 0.552833\n",
      "Epoch 27600, Train loss: 0.552828\n",
      "Epoch 27700, Train loss: 0.552823\n",
      "Epoch 27800, Train loss: 0.552818\n",
      "Epoch 27900, Train loss: 0.552814\n",
      "Epoch 28000, Train loss: 0.552809\n",
      "Epoch 28100, Train loss: 0.552804\n",
      "Epoch 28200, Train loss: 0.552799\n",
      "Epoch 28300, Train loss: 0.552795\n",
      "Epoch 28400, Train loss: 0.552790\n",
      "Epoch 28500, Train loss: 0.552785\n",
      "Epoch 28600, Train loss: 0.552781\n",
      "Epoch 28700, Train loss: 0.552776\n",
      "Epoch 28800, Train loss: 0.552772\n",
      "Epoch 28900, Train loss: 0.552767\n",
      "Epoch 29000, Train loss: 0.552763\n",
      "Epoch 29100, Train loss: 0.552758\n",
      "Epoch 29200, Train loss: 0.552754\n",
      "Epoch 29300, Train loss: 0.552750\n",
      "Epoch 29400, Train loss: 0.552745\n",
      "Epoch 29500, Train loss: 0.552741\n",
      "Epoch 29600, Train loss: 0.552736\n",
      "Epoch 29700, Train loss: 0.552732\n",
      "Epoch 29800, Train loss: 0.552728\n",
      "Epoch 29900, Train loss: 0.552724\n",
      "Epoch 30000, Train loss: 0.552720\n",
      "Epoch 30100, Train loss: 0.552715\n",
      "Epoch 30200, Train loss: 0.552711\n",
      "Epoch 30300, Train loss: 0.552707\n",
      "Epoch 30400, Train loss: 0.552703\n",
      "Epoch 30500, Train loss: 0.552699\n",
      "Epoch 30600, Train loss: 0.552695\n",
      "Epoch 30700, Train loss: 0.552691\n",
      "Epoch 30800, Train loss: 0.552687\n",
      "Epoch 30900, Train loss: 0.552683\n",
      "Epoch 31000, Train loss: 0.552679\n",
      "Epoch 31100, Train loss: 0.552675\n",
      "Epoch 31200, Train loss: 0.552671\n",
      "Epoch 31300, Train loss: 0.552668\n",
      "Epoch 31400, Train loss: 0.552664\n",
      "Epoch 31500, Train loss: 0.552660\n",
      "Epoch 31600, Train loss: 0.552656\n",
      "Epoch 31700, Train loss: 0.552652\n",
      "Epoch 31800, Train loss: 0.552649\n",
      "Epoch 31900, Train loss: 0.552645\n",
      "Epoch 32000, Train loss: 0.552641\n",
      "Epoch 32100, Train loss: 0.552638\n",
      "Epoch 32200, Train loss: 0.552634\n",
      "Epoch 32300, Train loss: 0.552630\n",
      "Epoch 32400, Train loss: 0.552627\n",
      "Epoch 32500, Train loss: 0.552623\n",
      "Epoch 32600, Train loss: 0.552620\n",
      "Epoch 32700, Train loss: 0.552616\n",
      "Epoch 32800, Train loss: 0.552613\n",
      "Epoch 32900, Train loss: 0.552609\n",
      "Epoch 33000, Train loss: 0.552606\n",
      "Epoch 33100, Train loss: 0.552602\n",
      "Epoch 33200, Train loss: 0.552599\n",
      "Epoch 33300, Train loss: 0.552595\n",
      "Epoch 33400, Train loss: 0.552592\n",
      "Epoch 33500, Train loss: 0.552589\n",
      "Epoch 33600, Train loss: 0.552585\n",
      "Epoch 33700, Train loss: 0.552582\n",
      "Epoch 33800, Train loss: 0.552579\n",
      "Epoch 33900, Train loss: 0.552575\n",
      "Epoch 34000, Train loss: 0.552572\n",
      "Epoch 34100, Train loss: 0.552569\n",
      "Epoch 34200, Train loss: 0.552566\n",
      "Epoch 34300, Train loss: 0.552562\n",
      "Epoch 34400, Train loss: 0.552559\n",
      "Epoch 34500, Train loss: 0.552556\n",
      "Epoch 34600, Train loss: 0.552553\n",
      "Epoch 34700, Train loss: 0.552550\n",
      "Epoch 34800, Train loss: 0.552547\n",
      "Epoch 34900, Train loss: 0.552544\n",
      "Epoch 35000, Train loss: 0.552540\n",
      "Epoch 35100, Train loss: 0.552537\n",
      "Epoch 35200, Train loss: 0.552534\n",
      "Epoch 35300, Train loss: 0.552531\n",
      "Epoch 35400, Train loss: 0.552528\n",
      "Epoch 35500, Train loss: 0.552525\n",
      "Epoch 35600, Train loss: 0.552522\n",
      "Epoch 35700, Train loss: 0.552519\n",
      "Epoch 35800, Train loss: 0.552516\n",
      "Epoch 35900, Train loss: 0.552513\n",
      "Epoch 36000, Train loss: 0.552510\n",
      "Epoch 36100, Train loss: 0.552508\n",
      "Epoch 36200, Train loss: 0.552505\n",
      "Epoch 36300, Train loss: 0.552502\n",
      "Epoch 36400, Train loss: 0.552499\n",
      "Epoch 36500, Train loss: 0.552496\n",
      "Epoch 36600, Train loss: 0.552493\n",
      "Epoch 36700, Train loss: 0.552490\n",
      "Epoch 36800, Train loss: 0.552488\n",
      "Epoch 36900, Train loss: 0.552485\n",
      "Epoch 37000, Train loss: 0.552482\n",
      "Epoch 37100, Train loss: 0.552479\n",
      "Epoch 37200, Train loss: 0.552477\n",
      "Epoch 37300, Train loss: 0.552474\n",
      "Epoch 37400, Train loss: 0.552471\n",
      "Epoch 37500, Train loss: 0.552468\n",
      "Epoch 37600, Train loss: 0.552466\n",
      "Epoch 37700, Train loss: 0.552463\n",
      "Epoch 37800, Train loss: 0.552461\n",
      "Epoch 37900, Train loss: 0.552458\n",
      "Epoch 38000, Train loss: 0.552455\n",
      "Epoch 38100, Train loss: 0.552453\n",
      "Epoch 38200, Train loss: 0.552450\n",
      "Epoch 38300, Train loss: 0.552447\n",
      "Epoch 38400, Train loss: 0.552445\n",
      "Epoch 38500, Train loss: 0.552442\n",
      "Epoch 38600, Train loss: 0.552440\n",
      "Epoch 38700, Train loss: 0.552437\n",
      "Epoch 38800, Train loss: 0.552435\n",
      "Epoch 38900, Train loss: 0.552432\n",
      "Epoch 39000, Train loss: 0.552430\n",
      "Epoch 39100, Train loss: 0.552427\n",
      "Epoch 39200, Train loss: 0.552425\n",
      "Epoch 39300, Train loss: 0.552422\n",
      "Epoch 39400, Train loss: 0.552420\n",
      "Epoch 39500, Train loss: 0.552417\n",
      "Epoch 39600, Train loss: 0.552415\n",
      "Epoch 39700, Train loss: 0.552413\n",
      "Epoch 39800, Train loss: 0.552410\n",
      "Epoch 39900, Train loss: 0.552408\n",
      "Epoch 40000, Train loss: 0.552405\n",
      "Epoch 40100, Train loss: 0.552403\n",
      "Epoch 40200, Train loss: 0.552401\n",
      "Epoch 40300, Train loss: 0.552398\n",
      "Epoch 40400, Train loss: 0.552396\n",
      "Epoch 40500, Train loss: 0.552394\n",
      "Epoch 40600, Train loss: 0.552391\n",
      "Epoch 40700, Train loss: 0.552389\n",
      "Epoch 40800, Train loss: 0.552387\n",
      "Epoch 40900, Train loss: 0.552385\n",
      "Epoch 41000, Train loss: 0.552382\n",
      "Epoch 41100, Train loss: 0.552380\n",
      "Epoch 41200, Train loss: 0.552378\n",
      "Epoch 41300, Train loss: 0.552376\n",
      "Epoch 41400, Train loss: 0.552373\n",
      "Epoch 41500, Train loss: 0.552371\n",
      "Epoch 41600, Train loss: 0.552369\n",
      "Epoch 41700, Train loss: 0.552367\n",
      "Epoch 41800, Train loss: 0.552365\n",
      "Epoch 41900, Train loss: 0.552362\n",
      "Epoch 42000, Train loss: 0.552360\n",
      "Epoch 42100, Train loss: 0.552358\n",
      "Epoch 42200, Train loss: 0.552356\n",
      "Epoch 42300, Train loss: 0.552354\n",
      "Epoch 42400, Train loss: 0.552352\n",
      "Epoch 42500, Train loss: 0.552350\n",
      "Epoch 42600, Train loss: 0.552348\n",
      "Epoch 42700, Train loss: 0.552346\n",
      "Epoch 42800, Train loss: 0.552343\n",
      "Epoch 42900, Train loss: 0.552341\n",
      "Epoch 43000, Train loss: 0.552339\n",
      "Epoch 43100, Train loss: 0.552337\n",
      "Epoch 43200, Train loss: 0.552335\n",
      "Epoch 43300, Train loss: 0.552333\n",
      "Epoch 43400, Train loss: 0.552331\n",
      "Epoch 43500, Train loss: 0.552329\n",
      "Epoch 43600, Train loss: 0.552327\n",
      "Epoch 43700, Train loss: 0.552325\n",
      "Epoch 43800, Train loss: 0.552323\n",
      "Epoch 43900, Train loss: 0.552321\n",
      "Epoch 44000, Train loss: 0.552319\n",
      "Epoch 44100, Train loss: 0.552317\n",
      "Epoch 44200, Train loss: 0.552315\n",
      "Epoch 44300, Train loss: 0.552313\n",
      "Epoch 44400, Train loss: 0.552311\n",
      "Epoch 44500, Train loss: 0.552310\n",
      "Epoch 44600, Train loss: 0.552308\n",
      "Epoch 44700, Train loss: 0.552306\n",
      "Epoch 44800, Train loss: 0.552304\n",
      "Epoch 44900, Train loss: 0.552302\n",
      "Epoch 45000, Train loss: 0.552300\n",
      "Epoch 45100, Train loss: 0.552298\n",
      "Epoch 45200, Train loss: 0.552296\n",
      "Epoch 45300, Train loss: 0.552294\n",
      "Epoch 45400, Train loss: 0.552293\n",
      "Epoch 45500, Train loss: 0.552291\n",
      "Epoch 45600, Train loss: 0.552289\n",
      "Epoch 45700, Train loss: 0.552287\n",
      "Epoch 45800, Train loss: 0.552285\n",
      "Epoch 45900, Train loss: 0.552284\n",
      "Epoch 46000, Train loss: 0.552282\n",
      "Epoch 46100, Train loss: 0.552280\n",
      "Epoch 46200, Train loss: 0.552278\n",
      "Epoch 46300, Train loss: 0.552276\n",
      "Epoch 46400, Train loss: 0.552275\n",
      "Epoch 46500, Train loss: 0.552273\n",
      "Epoch 46600, Train loss: 0.552271\n",
      "Epoch 46700, Train loss: 0.552269\n",
      "Epoch 46800, Train loss: 0.552268\n",
      "Epoch 46900, Train loss: 0.552266\n",
      "Epoch 47000, Train loss: 0.552264\n",
      "Epoch 47100, Train loss: 0.552262\n",
      "Epoch 47200, Train loss: 0.552261\n",
      "Epoch 47300, Train loss: 0.552259\n",
      "Epoch 47400, Train loss: 0.552257\n",
      "Epoch 47500, Train loss: 0.552256\n",
      "Epoch 47600, Train loss: 0.552254\n",
      "Epoch 47700, Train loss: 0.552252\n",
      "Epoch 47800, Train loss: 0.552251\n",
      "Epoch 47900, Train loss: 0.552249\n",
      "Epoch 48000, Train loss: 0.552247\n",
      "Epoch 48100, Train loss: 0.552246\n",
      "Epoch 48200, Train loss: 0.552244\n",
      "Epoch 48300, Train loss: 0.552242\n",
      "Epoch 48400, Train loss: 0.552241\n",
      "Epoch 48500, Train loss: 0.552239\n",
      "Epoch 48600, Train loss: 0.552237\n",
      "Epoch 48700, Train loss: 0.552236\n",
      "Epoch 48800, Train loss: 0.552234\n",
      "Epoch 48900, Train loss: 0.552233\n",
      "Epoch 49000, Train loss: 0.552231\n",
      "Epoch 49100, Train loss: 0.552229\n",
      "Epoch 49200, Train loss: 0.552228\n",
      "Epoch 49300, Train loss: 0.552226\n",
      "Epoch 49400, Train loss: 0.552225\n",
      "Epoch 49500, Train loss: 0.552223\n",
      "Epoch 49600, Train loss: 0.552222\n",
      "Epoch 49700, Train loss: 0.552220\n",
      "Epoch 49800, Train loss: 0.552219\n",
      "Epoch 49900, Train loss: 0.552217\n",
      "Epoch 50000, Train loss: 0.552216\n",
      "Epoch 50100, Train loss: 0.552214\n",
      "Epoch 50200, Train loss: 0.552213\n",
      "Epoch 50300, Train loss: 0.552211\n",
      "Epoch 50400, Train loss: 0.552209\n",
      "Epoch 50500, Train loss: 0.552208\n",
      "Epoch 50600, Train loss: 0.552207\n",
      "Epoch 50700, Train loss: 0.552205\n",
      "Epoch 50800, Train loss: 0.552204\n",
      "Epoch 50900, Train loss: 0.552202\n",
      "Epoch 51000, Train loss: 0.552201\n",
      "Epoch 51100, Train loss: 0.552199\n",
      "Epoch 51200, Train loss: 0.552198\n",
      "Epoch 51300, Train loss: 0.552196\n",
      "Epoch 51400, Train loss: 0.552195\n",
      "Epoch 51500, Train loss: 0.552193\n",
      "Epoch 51600, Train loss: 0.552192\n",
      "Epoch 51700, Train loss: 0.552190\n",
      "Epoch 51800, Train loss: 0.552189\n",
      "Epoch 51900, Train loss: 0.552188\n",
      "Epoch 52000, Train loss: 0.552186\n",
      "Epoch 52100, Train loss: 0.552185\n",
      "Epoch 52200, Train loss: 0.552183\n",
      "Epoch 52300, Train loss: 0.552182\n",
      "Epoch 52400, Train loss: 0.552181\n",
      "Epoch 52500, Train loss: 0.552179\n",
      "Epoch 52600, Train loss: 0.552178\n",
      "Epoch 52700, Train loss: 0.552176\n",
      "Epoch 52800, Train loss: 0.552175\n",
      "Epoch 52900, Train loss: 0.552174\n",
      "Epoch 53000, Train loss: 0.552172\n",
      "Epoch 53100, Train loss: 0.552171\n",
      "Epoch 53200, Train loss: 0.552170\n",
      "Epoch 53300, Train loss: 0.552168\n",
      "Epoch 53400, Train loss: 0.552167\n",
      "Epoch 53500, Train loss: 0.552166\n",
      "Epoch 53600, Train loss: 0.552164\n",
      "Epoch 53700, Train loss: 0.552163\n",
      "Epoch 53800, Train loss: 0.552162\n",
      "Epoch 53900, Train loss: 0.552160\n",
      "Epoch 54000, Train loss: 0.552159\n",
      "Epoch 54100, Train loss: 0.552158\n",
      "Epoch 54200, Train loss: 0.552156\n",
      "Epoch 54300, Train loss: 0.552155\n",
      "Epoch 54400, Train loss: 0.552154\n",
      "Epoch 54500, Train loss: 0.552153\n",
      "Epoch 54600, Train loss: 0.552151\n",
      "Epoch 54700, Train loss: 0.552150\n",
      "Epoch 54800, Train loss: 0.552149\n",
      "Epoch 54900, Train loss: 0.552148\n",
      "Epoch 55000, Train loss: 0.552146\n",
      "Epoch 55100, Train loss: 0.552145\n",
      "Epoch 55200, Train loss: 0.552144\n",
      "Epoch 55300, Train loss: 0.552142\n",
      "Epoch 55400, Train loss: 0.552141\n",
      "Epoch 55500, Train loss: 0.552140\n",
      "Epoch 55600, Train loss: 0.552139\n",
      "Epoch 55700, Train loss: 0.552137\n",
      "Epoch 55800, Train loss: 0.552136\n",
      "Epoch 55900, Train loss: 0.552135\n",
      "Epoch 56000, Train loss: 0.552134\n",
      "Epoch 56100, Train loss: 0.552132\n",
      "Epoch 56200, Train loss: 0.552131\n",
      "Epoch 56300, Train loss: 0.552130\n",
      "Epoch 56400, Train loss: 0.552129\n",
      "Epoch 56500, Train loss: 0.552128\n",
      "Epoch 56600, Train loss: 0.552127\n",
      "Epoch 56700, Train loss: 0.552125\n",
      "Epoch 56800, Train loss: 0.552124\n",
      "Epoch 56900, Train loss: 0.552123\n",
      "Epoch 57000, Train loss: 0.552122\n",
      "Epoch 57100, Train loss: 0.552121\n",
      "Epoch 57200, Train loss: 0.552119\n",
      "Epoch 57300, Train loss: 0.552118\n",
      "Epoch 57400, Train loss: 0.552117\n",
      "Epoch 57500, Train loss: 0.552116\n",
      "Epoch 57600, Train loss: 0.552115\n",
      "Epoch 57700, Train loss: 0.552114\n",
      "Epoch 57800, Train loss: 0.552113\n",
      "Epoch 57900, Train loss: 0.552111\n",
      "Epoch 58000, Train loss: 0.552110\n",
      "Epoch 58100, Train loss: 0.552109\n",
      "Epoch 58200, Train loss: 0.552108\n",
      "Epoch 58300, Train loss: 0.552107\n",
      "Epoch 58400, Train loss: 0.552106\n",
      "Epoch 58500, Train loss: 0.552105\n",
      "Epoch 58600, Train loss: 0.552104\n",
      "Epoch 58700, Train loss: 0.552102\n",
      "Epoch 58800, Train loss: 0.552101\n",
      "Epoch 58900, Train loss: 0.552100\n",
      "Epoch 59000, Train loss: 0.552099\n",
      "Epoch 59100, Train loss: 0.552098\n",
      "Epoch 59200, Train loss: 0.552097\n",
      "Epoch 59300, Train loss: 0.552096\n",
      "Epoch 59400, Train loss: 0.552095\n",
      "Epoch 59500, Train loss: 0.552094\n",
      "Epoch 59600, Train loss: 0.552093\n",
      "Epoch 59700, Train loss: 0.552091\n",
      "Epoch 59800, Train loss: 0.552090\n",
      "Epoch 59900, Train loss: 0.552089\n",
      "Epoch 60000, Train loss: 0.552088\n",
      "Epoch 60100, Train loss: 0.552087\n",
      "Epoch 60200, Train loss: 0.552086\n",
      "Epoch 60300, Train loss: 0.552085\n",
      "Epoch 60400, Train loss: 0.552084\n",
      "Epoch 60500, Train loss: 0.552083\n",
      "Epoch 60600, Train loss: 0.552082\n",
      "Epoch 60700, Train loss: 0.552081\n",
      "Epoch 60800, Train loss: 0.552080\n",
      "Epoch 60900, Train loss: 0.552079\n",
      "Epoch 61000, Train loss: 0.552078\n",
      "Epoch 61100, Train loss: 0.552077\n",
      "Epoch 61200, Train loss: 0.552076\n",
      "Epoch 61300, Train loss: 0.552075\n",
      "Epoch 61400, Train loss: 0.552074\n",
      "Epoch 61500, Train loss: 0.552073\n",
      "Epoch 61600, Train loss: 0.552072\n",
      "Epoch 61700, Train loss: 0.552071\n",
      "Epoch 61800, Train loss: 0.552070\n",
      "Epoch 61900, Train loss: 0.552069\n",
      "Epoch 62000, Train loss: 0.552068\n",
      "Epoch 62100, Train loss: 0.552067\n",
      "Epoch 62200, Train loss: 0.552066\n",
      "Epoch 62300, Train loss: 0.552065\n",
      "Epoch 62400, Train loss: 0.552064\n",
      "Epoch 62500, Train loss: 0.552063\n",
      "Epoch 62600, Train loss: 0.552062\n",
      "Epoch 62700, Train loss: 0.552061\n",
      "Epoch 62800, Train loss: 0.552060\n",
      "Epoch 62900, Train loss: 0.552059\n",
      "Epoch 63000, Train loss: 0.552058\n",
      "Epoch 63100, Train loss: 0.552057\n",
      "Epoch 63200, Train loss: 0.552056\n",
      "Epoch 63300, Train loss: 0.552055\n",
      "Epoch 63400, Train loss: 0.552054\n",
      "Epoch 63500, Train loss: 0.552053\n",
      "Epoch 63600, Train loss: 0.552052\n",
      "Epoch 63700, Train loss: 0.552051\n",
      "Epoch 63800, Train loss: 0.552050\n",
      "Epoch 63900, Train loss: 0.552049\n",
      "Epoch 64000, Train loss: 0.552048\n",
      "Epoch 64100, Train loss: 0.552047\n",
      "Epoch 64200, Train loss: 0.552047\n",
      "Epoch 64300, Train loss: 0.552046\n",
      "Epoch 64400, Train loss: 0.552045\n",
      "Epoch 64500, Train loss: 0.552044\n",
      "Epoch 64600, Train loss: 0.552043\n",
      "Epoch 64700, Train loss: 0.552042\n",
      "Epoch 64800, Train loss: 0.552041\n",
      "Epoch 64900, Train loss: 0.552040\n",
      "Epoch 65000, Train loss: 0.552039\n",
      "Epoch 65100, Train loss: 0.552038\n",
      "Epoch 65200, Train loss: 0.552037\n",
      "Epoch 65300, Train loss: 0.552037\n",
      "Epoch 65400, Train loss: 0.552036\n",
      "Epoch 65500, Train loss: 0.552035\n",
      "Epoch 65600, Train loss: 0.552034\n",
      "Epoch 65700, Train loss: 0.552033\n",
      "Epoch 65800, Train loss: 0.552032\n",
      "Epoch 65900, Train loss: 0.552031\n",
      "Epoch 66000, Train loss: 0.552030\n",
      "Epoch 66100, Train loss: 0.552029\n",
      "Epoch 66200, Train loss: 0.552029\n",
      "Epoch 66300, Train loss: 0.552028\n",
      "Epoch 66400, Train loss: 0.552027\n",
      "Epoch 66500, Train loss: 0.552026\n",
      "Epoch 66600, Train loss: 0.552025\n",
      "Epoch 66700, Train loss: 0.552024\n",
      "Epoch 66800, Train loss: 0.552023\n",
      "Epoch 66900, Train loss: 0.552022\n",
      "Epoch 67000, Train loss: 0.552022\n",
      "Epoch 67100, Train loss: 0.552021\n",
      "Epoch 67200, Train loss: 0.552020\n",
      "Epoch 67300, Train loss: 0.552019\n",
      "Epoch 67400, Train loss: 0.552018\n",
      "Epoch 67500, Train loss: 0.552017\n",
      "Epoch 67600, Train loss: 0.552017\n",
      "Epoch 67700, Train loss: 0.552016\n",
      "Epoch 67800, Train loss: 0.552015\n",
      "Epoch 67900, Train loss: 0.552014\n",
      "Epoch 68000, Train loss: 0.552013\n",
      "Epoch 68100, Train loss: 0.552012\n",
      "Epoch 68200, Train loss: 0.552012\n",
      "Epoch 68300, Train loss: 0.552011\n",
      "Epoch 68400, Train loss: 0.552010\n",
      "Epoch 68500, Train loss: 0.552009\n",
      "Epoch 68600, Train loss: 0.552008\n",
      "Epoch 68700, Train loss: 0.552007\n",
      "Epoch 68800, Train loss: 0.552007\n",
      "Epoch 68900, Train loss: 0.552006\n",
      "Epoch 69000, Train loss: 0.552005\n",
      "Epoch 69100, Train loss: 0.552004\n",
      "Epoch 69200, Train loss: 0.552003\n",
      "Epoch 69300, Train loss: 0.552003\n",
      "Epoch 69400, Train loss: 0.552002\n",
      "Epoch 69500, Train loss: 0.552001\n",
      "Epoch 69600, Train loss: 0.552000\n",
      "Epoch 69700, Train loss: 0.551999\n",
      "Epoch 69800, Train loss: 0.551999\n",
      "Epoch 69900, Train loss: 0.551998\n",
      "Epoch 70000, Train loss: 0.551997\n",
      "Epoch 70100, Train loss: 0.551996\n",
      "Epoch 70200, Train loss: 0.551996\n",
      "Epoch 70300, Train loss: 0.551995\n",
      "Epoch 70400, Train loss: 0.551994\n",
      "Epoch 70500, Train loss: 0.551993\n",
      "Epoch 70600, Train loss: 0.551992\n",
      "Epoch 70700, Train loss: 0.551992\n",
      "Epoch 70800, Train loss: 0.551991\n",
      "Epoch 70900, Train loss: 0.551990\n",
      "Epoch 71000, Train loss: 0.551989\n",
      "Epoch 71100, Train loss: 0.551989\n",
      "Epoch 71200, Train loss: 0.551988\n",
      "Epoch 71300, Train loss: 0.551987\n",
      "Epoch 71400, Train loss: 0.551986\n",
      "Epoch 71500, Train loss: 0.551986\n",
      "Epoch 71600, Train loss: 0.551985\n",
      "Epoch 71700, Train loss: 0.551984\n",
      "Epoch 71800, Train loss: 0.551983\n",
      "Epoch 71900, Train loss: 0.551983\n",
      "Epoch 72000, Train loss: 0.551982\n",
      "Epoch 72100, Train loss: 0.551981\n",
      "Epoch 72200, Train loss: 0.551980\n",
      "Epoch 72300, Train loss: 0.551980\n",
      "Epoch 72400, Train loss: 0.551979\n",
      "Epoch 72500, Train loss: 0.551978\n",
      "Epoch 72600, Train loss: 0.551978\n",
      "Epoch 72700, Train loss: 0.551977\n",
      "Epoch 72800, Train loss: 0.551976\n",
      "Epoch 72900, Train loss: 0.551975\n",
      "Epoch 73000, Train loss: 0.551975\n",
      "Epoch 73100, Train loss: 0.551974\n",
      "Epoch 73200, Train loss: 0.551973\n",
      "Epoch 73300, Train loss: 0.551972\n",
      "Epoch 73400, Train loss: 0.551972\n",
      "Epoch 73500, Train loss: 0.551971\n",
      "Epoch 73600, Train loss: 0.551970\n",
      "Epoch 73700, Train loss: 0.551970\n",
      "Epoch 73800, Train loss: 0.551969\n",
      "Epoch 73900, Train loss: 0.551968\n",
      "Epoch 74000, Train loss: 0.551968\n",
      "Epoch 74100, Train loss: 0.551967\n",
      "Epoch 74200, Train loss: 0.551966\n",
      "Epoch 74300, Train loss: 0.551965\n",
      "Epoch 74400, Train loss: 0.551965\n",
      "Epoch 74500, Train loss: 0.551964\n",
      "Epoch 74600, Train loss: 0.551963\n",
      "Epoch 74700, Train loss: 0.551963\n",
      "Epoch 74800, Train loss: 0.551962\n",
      "Epoch 74900, Train loss: 0.551961\n",
      "Epoch 75000, Train loss: 0.551961\n",
      "Epoch 75100, Train loss: 0.551960\n",
      "Epoch 75200, Train loss: 0.551959\n",
      "Epoch 75300, Train loss: 0.551958\n",
      "Epoch 75400, Train loss: 0.551958\n",
      "Epoch 75500, Train loss: 0.551957\n",
      "Epoch 75600, Train loss: 0.551956\n",
      "Epoch 75700, Train loss: 0.551956\n",
      "Epoch 75800, Train loss: 0.551955\n",
      "Epoch 75900, Train loss: 0.551955\n",
      "Epoch 76000, Train loss: 0.551954\n",
      "Epoch 76100, Train loss: 0.551953\n",
      "Epoch 76200, Train loss: 0.551952\n",
      "Epoch 76300, Train loss: 0.551952\n",
      "Epoch 76400, Train loss: 0.551951\n",
      "Epoch 76500, Train loss: 0.551951\n",
      "Epoch 76600, Train loss: 0.551950\n",
      "Epoch 76700, Train loss: 0.551949\n",
      "Epoch 76800, Train loss: 0.551949\n",
      "Epoch 76900, Train loss: 0.551948\n",
      "Epoch 77000, Train loss: 0.551947\n",
      "Epoch 77100, Train loss: 0.551947\n",
      "Epoch 77200, Train loss: 0.551946\n",
      "Epoch 77300, Train loss: 0.551945\n",
      "Epoch 77400, Train loss: 0.551945\n",
      "Epoch 77500, Train loss: 0.551944\n",
      "Epoch 77600, Train loss: 0.551943\n",
      "Epoch 77700, Train loss: 0.551943\n",
      "Epoch 77800, Train loss: 0.551942\n",
      "Epoch 77900, Train loss: 0.551942\n",
      "Epoch 78000, Train loss: 0.551941\n",
      "Epoch 78100, Train loss: 0.551940\n",
      "Epoch 78200, Train loss: 0.551940\n",
      "Epoch 78300, Train loss: 0.551939\n",
      "Epoch 78400, Train loss: 0.551938\n",
      "Epoch 78500, Train loss: 0.551938\n",
      "Epoch 78600, Train loss: 0.551937\n",
      "Epoch 78700, Train loss: 0.551937\n",
      "Epoch 78800, Train loss: 0.551936\n",
      "Epoch 78900, Train loss: 0.551935\n",
      "Epoch 79000, Train loss: 0.551935\n",
      "Epoch 79100, Train loss: 0.551934\n",
      "Epoch 79200, Train loss: 0.551933\n",
      "Epoch 79300, Train loss: 0.551933\n",
      "Epoch 79400, Train loss: 0.551932\n",
      "Epoch 79500, Train loss: 0.551932\n",
      "Epoch 79600, Train loss: 0.551931\n",
      "Epoch 79700, Train loss: 0.551930\n",
      "Epoch 79800, Train loss: 0.551930\n",
      "Epoch 79900, Train loss: 0.551929\n",
      "Epoch 80000, Train loss: 0.551929\n",
      "Epoch 80100, Train loss: 0.551928\n",
      "Epoch 80200, Train loss: 0.551927\n",
      "Epoch 80300, Train loss: 0.551927\n",
      "Epoch 80400, Train loss: 0.551926\n",
      "Epoch 80500, Train loss: 0.551926\n",
      "Epoch 80600, Train loss: 0.551925\n",
      "Epoch 80700, Train loss: 0.551924\n",
      "Epoch 80800, Train loss: 0.551924\n",
      "Epoch 80900, Train loss: 0.551923\n",
      "Epoch 81000, Train loss: 0.551923\n",
      "Epoch 81100, Train loss: 0.551922\n",
      "Epoch 81200, Train loss: 0.551921\n",
      "Epoch 81300, Train loss: 0.551921\n",
      "Epoch 81400, Train loss: 0.551920\n",
      "Epoch 81500, Train loss: 0.551920\n",
      "Epoch 81600, Train loss: 0.551919\n",
      "Epoch 81700, Train loss: 0.551919\n",
      "Epoch 81800, Train loss: 0.551918\n",
      "Epoch 81900, Train loss: 0.551917\n",
      "Epoch 82000, Train loss: 0.551917\n",
      "Epoch 82100, Train loss: 0.551916\n",
      "Epoch 82200, Train loss: 0.551916\n",
      "Epoch 82300, Train loss: 0.551915\n",
      "Epoch 82400, Train loss: 0.551915\n",
      "Epoch 82500, Train loss: 0.551914\n",
      "Epoch 82600, Train loss: 0.551913\n",
      "Epoch 82700, Train loss: 0.551913\n",
      "Epoch 82800, Train loss: 0.551912\n",
      "Epoch 82900, Train loss: 0.551912\n",
      "Epoch 83000, Train loss: 0.551911\n",
      "Epoch 83100, Train loss: 0.551911\n",
      "Epoch 83200, Train loss: 0.551910\n",
      "Epoch 83300, Train loss: 0.551910\n",
      "Epoch 83400, Train loss: 0.551909\n",
      "Epoch 83500, Train loss: 0.551908\n",
      "Epoch 83600, Train loss: 0.551908\n",
      "Epoch 83700, Train loss: 0.551907\n",
      "Epoch 83800, Train loss: 0.551907\n",
      "Epoch 83900, Train loss: 0.551906\n",
      "Epoch 84000, Train loss: 0.551906\n",
      "Epoch 84100, Train loss: 0.551905\n",
      "Epoch 84200, Train loss: 0.551905\n",
      "Epoch 84300, Train loss: 0.551904\n",
      "Epoch 84400, Train loss: 0.551904\n",
      "Epoch 84500, Train loss: 0.551903\n",
      "Epoch 84600, Train loss: 0.551903\n",
      "Epoch 84700, Train loss: 0.551902\n",
      "Epoch 84800, Train loss: 0.551901\n",
      "Epoch 84900, Train loss: 0.551901\n",
      "Epoch 85000, Train loss: 0.551900\n",
      "Epoch 85100, Train loss: 0.551900\n",
      "Epoch 85200, Train loss: 0.551899\n",
      "Epoch 85300, Train loss: 0.551899\n",
      "Epoch 85400, Train loss: 0.551898\n",
      "Epoch 85500, Train loss: 0.551898\n",
      "Epoch 85600, Train loss: 0.551897\n",
      "Epoch 85700, Train loss: 0.551897\n",
      "Epoch 85800, Train loss: 0.551896\n",
      "Epoch 85900, Train loss: 0.551895\n",
      "Epoch 86000, Train loss: 0.551895\n",
      "Epoch 86100, Train loss: 0.551894\n",
      "Epoch 86200, Train loss: 0.551894\n",
      "Epoch 86300, Train loss: 0.551893\n",
      "Epoch 86400, Train loss: 0.551893\n",
      "Epoch 86500, Train loss: 0.551892\n",
      "Epoch 86600, Train loss: 0.551892\n",
      "Epoch 86700, Train loss: 0.551891\n",
      "Epoch 86800, Train loss: 0.551891\n",
      "Epoch 86900, Train loss: 0.551890\n",
      "Epoch 87000, Train loss: 0.551890\n",
      "Epoch 87100, Train loss: 0.551889\n",
      "Epoch 87200, Train loss: 0.551889\n",
      "Epoch 87300, Train loss: 0.551888\n",
      "Epoch 87400, Train loss: 0.551888\n",
      "Epoch 87500, Train loss: 0.551887\n",
      "Epoch 87600, Train loss: 0.551887\n",
      "Epoch 87700, Train loss: 0.551886\n",
      "Epoch 87800, Train loss: 0.551886\n",
      "Epoch 87900, Train loss: 0.551885\n",
      "Epoch 88000, Train loss: 0.551885\n",
      "Epoch 88100, Train loss: 0.551884\n",
      "Epoch 88200, Train loss: 0.551884\n",
      "Epoch 88300, Train loss: 0.551883\n",
      "Epoch 88400, Train loss: 0.551883\n",
      "Epoch 88500, Train loss: 0.551883\n",
      "Epoch 88600, Train loss: 0.551882\n",
      "Epoch 88700, Train loss: 0.551882\n",
      "Epoch 88800, Train loss: 0.551881\n",
      "Epoch 88900, Train loss: 0.551881\n",
      "Epoch 89000, Train loss: 0.551880\n",
      "Epoch 89100, Train loss: 0.551880\n",
      "Epoch 89200, Train loss: 0.551879\n",
      "Epoch 89300, Train loss: 0.551879\n",
      "Epoch 89400, Train loss: 0.551878\n",
      "Epoch 89500, Train loss: 0.551878\n",
      "Epoch 89600, Train loss: 0.551877\n",
      "Epoch 89700, Train loss: 0.551877\n",
      "Epoch 89800, Train loss: 0.551876\n",
      "Epoch 89900, Train loss: 0.551876\n",
      "Epoch 90000, Train loss: 0.551875\n",
      "Epoch 90100, Train loss: 0.551875\n",
      "Epoch 90200, Train loss: 0.551874\n",
      "Epoch 90300, Train loss: 0.551874\n",
      "Epoch 90400, Train loss: 0.551873\n",
      "Epoch 90500, Train loss: 0.551873\n",
      "Epoch 90600, Train loss: 0.551872\n",
      "Epoch 90700, Train loss: 0.551872\n",
      "Epoch 90800, Train loss: 0.551871\n",
      "Epoch 90900, Train loss: 0.551871\n",
      "Epoch 91000, Train loss: 0.551870\n",
      "Epoch 91100, Train loss: 0.551870\n",
      "Epoch 91200, Train loss: 0.551870\n",
      "Epoch 91300, Train loss: 0.551869\n",
      "Epoch 91400, Train loss: 0.551869\n",
      "Epoch 91500, Train loss: 0.551868\n",
      "Epoch 91600, Train loss: 0.551868\n",
      "Epoch 91700, Train loss: 0.551867\n",
      "Epoch 91800, Train loss: 0.551867\n",
      "Epoch 91900, Train loss: 0.551866\n",
      "Epoch 92000, Train loss: 0.551866\n",
      "Epoch 92100, Train loss: 0.551865\n",
      "Epoch 92200, Train loss: 0.551865\n",
      "Epoch 92300, Train loss: 0.551864\n",
      "Epoch 92400, Train loss: 0.551864\n",
      "Epoch 92500, Train loss: 0.551864\n",
      "Epoch 92600, Train loss: 0.551863\n",
      "Epoch 92700, Train loss: 0.551863\n",
      "Epoch 92800, Train loss: 0.551862\n",
      "Epoch 92900, Train loss: 0.551862\n",
      "Epoch 93000, Train loss: 0.551861\n",
      "Epoch 93100, Train loss: 0.551861\n",
      "Epoch 93200, Train loss: 0.551861\n",
      "Epoch 93300, Train loss: 0.551860\n",
      "Epoch 93400, Train loss: 0.551860\n",
      "Epoch 93500, Train loss: 0.551859\n",
      "Epoch 93600, Train loss: 0.551859\n",
      "Epoch 93700, Train loss: 0.551858\n",
      "Epoch 93800, Train loss: 0.551858\n",
      "Epoch 93900, Train loss: 0.551857\n",
      "Epoch 94000, Train loss: 0.551857\n",
      "Epoch 94100, Train loss: 0.551857\n",
      "Epoch 94200, Train loss: 0.551856\n",
      "Epoch 94300, Train loss: 0.551856\n",
      "Epoch 94400, Train loss: 0.551855\n",
      "Epoch 94500, Train loss: 0.551855\n",
      "Epoch 94600, Train loss: 0.551854\n",
      "Epoch 94700, Train loss: 0.551854\n",
      "Epoch 94800, Train loss: 0.551854\n",
      "Epoch 94900, Train loss: 0.551853\n",
      "Epoch 95000, Train loss: 0.551853\n",
      "Epoch 95100, Train loss: 0.551852\n",
      "Epoch 95200, Train loss: 0.551852\n",
      "Epoch 95300, Train loss: 0.551852\n",
      "Epoch 95400, Train loss: 0.551851\n",
      "Epoch 95500, Train loss: 0.551851\n",
      "Epoch 95600, Train loss: 0.551850\n",
      "Epoch 95700, Train loss: 0.551850\n",
      "Epoch 95800, Train loss: 0.551849\n",
      "Epoch 95900, Train loss: 0.551849\n",
      "Epoch 96000, Train loss: 0.551849\n",
      "Epoch 96100, Train loss: 0.551848\n",
      "Epoch 96200, Train loss: 0.551848\n",
      "Epoch 96300, Train loss: 0.551847\n",
      "Epoch 96400, Train loss: 0.551847\n",
      "Epoch 96500, Train loss: 0.551846\n",
      "Epoch 96600, Train loss: 0.551846\n",
      "Epoch 96700, Train loss: 0.551846\n",
      "Epoch 96800, Train loss: 0.551845\n",
      "Epoch 96900, Train loss: 0.551845\n",
      "Epoch 97000, Train loss: 0.551844\n",
      "Epoch 97100, Train loss: 0.551844\n",
      "Epoch 97200, Train loss: 0.551844\n",
      "Epoch 97300, Train loss: 0.551843\n",
      "Epoch 97400, Train loss: 0.551843\n",
      "Epoch 97500, Train loss: 0.551842\n",
      "Epoch 97600, Train loss: 0.551842\n",
      "Epoch 97700, Train loss: 0.551842\n",
      "Epoch 97800, Train loss: 0.551841\n",
      "Epoch 97900, Train loss: 0.551841\n",
      "Epoch 98000, Train loss: 0.551840\n",
      "Epoch 98100, Train loss: 0.551840\n",
      "Epoch 98200, Train loss: 0.551839\n",
      "Epoch 98300, Train loss: 0.551839\n",
      "Epoch 98400, Train loss: 0.551839\n",
      "Epoch 98500, Train loss: 0.551838\n",
      "Epoch 98600, Train loss: 0.551838\n",
      "Epoch 98700, Train loss: 0.551838\n",
      "Epoch 98800, Train loss: 0.551837\n",
      "Epoch 98900, Train loss: 0.551837\n",
      "Epoch 99000, Train loss: 0.551836\n",
      "Epoch 99100, Train loss: 0.551836\n",
      "Epoch 99200, Train loss: 0.551836\n",
      "Epoch 99300, Train loss: 0.551835\n",
      "Epoch 99400, Train loss: 0.551835\n",
      "Epoch 99500, Train loss: 0.551834\n",
      "Epoch 99600, Train loss: 0.551834\n",
      "Epoch 99700, Train loss: 0.551834\n",
      "Epoch 99800, Train loss: 0.551833\n",
      "Epoch 99900, Train loss: 0.551833\n"
     ]
    }
   ],
   "source": [
    "# 训练神经网络\n",
    "for epoch in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = net(X)\n",
    "    loss = criterion(y_hat, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch %d, Train loss: %f\" % (epoch, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
